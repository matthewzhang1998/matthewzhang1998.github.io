---
layout: page
permalink: /publications/
title: publications
---

#### Preprints and submissions
1. Yunbum Kook, Matthew S. Zhang "Covariance estimation with Markov chain Monte Carlo". [<a href="https://arxiv.org/abs/2410.17147v1">arXiv</a>] [<a href="https://arxiv.org/pdf/2410.17147v1">PDF</a>]
2. Sinho Chewi, Atsushi Nitanda, Matthew S. Zhang "Uniform-in-N log-Sobolev inequality for the mean-field Langevin dynamics with convex energy". [<a href="https://arxiv.org/abs/2409.10440">arXiv</a>] [<a href="https://arxiv.org/pdf/2409.10440">PDF</a>]

#### Journal articles
1. Sinho Chewi, Murat A. Erdogdu, Mufan (Bill) Li, Ruoqi Shen, Matthew S. Zhang "Analysis of Langevin Monte Carlo from Poincaré to log-Sobolev", in *Foundations of Computational Mathematics* (2024). [<a href="https://arxiv.org/abs/2112.12662">arXiv</a>] [<a
href="https://arxiv.org/pdf/2112.12662">PDF</a>]

#### Conference papers
1. Yunbum Kook, Matthew S. Zhang "Rényi-infinity constrained sampling with d<sup>3</sup> membership queries", in the *ACM-SIAM Symposium on Discrete Algorithms* (SODA 2025). [<a href="https://arxiv.org/abs/2407.12967">arXiv</a>] [<a href="https://arxiv.org/pdf/2407.12967">PDF</a>]
2. Yunbum Kook, Santosh Vempala, Matthew S. Zhang "In-and-Out: Algorithmic Diffusion for Sampling Convex Bodies", in the *38th Conference on Neural Information Processing Systems* (NeurIPS 2024, Spotlight). [<a href="https://arxiv.org/abs/2405.01425">arXiv</a>] [<a href="https://arxiv.org/pdf/2405.01425">PDF</a>]
3. Yunboom Kook, Matthew S. Zhang, Sinho Chewi, Mufan (Bill) Li, Murat A. Erdogdu "Sampling from the mean-field stationary distribution", in the *37th Conference on Learning Theory* (COLT 2024). [<a href="https://arxiv.org/abs/2402.07355">arXiv</a>] [<a href="https://arxiv.org/pdf/2402.07355">PDF</a>]
4. Matthew S. Zhang, Sinho Chewi, Mufan (Bill) Li, Krishnakumar Balasubramanian, Murat A. Erdogdu "Improved discretization analysis for the underdamped Langevin Monte Carlo", in the *36th Conference on Learning Theory* (COLT 2023). [<a href="https://arxiv.org/abs/2302.08049">arXiv</a>][<a href="https://arxiv.org/pdf/2302.08049">PDF</a>]
5. Tom Huix, Matthew S. Zhang, Alain Durmus "Tight regret and complexity bounds for Thompson sampling via Langevin Monte Carlo", in the *26th International Conference on Artificial Intelligence and Statistics* (AISTATS 2023). [<a href="https://proceedings.mlr.press/v206/huix23a/huix23a.pdf">PDF</a>]
6. Krishnakumar Balasubramanian, Sinho Chewi, Murat A. Erdogdu, Mufan Li, Adil Salim, Matthew S.
Zhang "Towards a Theory of Non-Log-Concave Sampling: First-Order Stationarity Guarantees for Langevin Monte Carlo", in the *35th Conference on Learning Theory* (COLT 2022). [<a href="https://arxiv.org/abs/2202.05214">arXiv</a>] [<a href="https://arxiv.org/pdf/2202.05214">PDF</a>] 
7. Matthew S. Zhang, Murat A. Erdogdu, Animesh Garg "Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings", in the *36th AAAI Conference on Artificial Intelligence* (AAAI 2022). [<a href="https://arxiv.org/abs/2111.00185">arXiv</a>] [<a href="https://arxiv.org/pdf/2111.00185">PDF</a>] 
8. Murat A. Erdogdu, Rasa Hosseinzadeh, Matthew S. Zhang "Convergence of Langevin Monte Carlo in Chi-Squared and Rényi Divergence",in the *25th International Conference on Artificial Intelligence and Statistics* (AISTATS 2022). [<a href="https://arxiv.org/abs/2007.11612">arXiv</a>] [<a href="https://arxiv.org/pdf/2007.11612">PDF</a>] 


#### Pre-PhD conference, journal, and workshop papers
1. Matthew S. Zhang, Bradly Stadie, "One-shot pruning of recurrent neural networks by Jacobian spectrum evaluation", in the *8th International Conference on Learning Representations* (ICLR 2020). [<a href="https://arxiv.org/abs/1912.00120">arXiv</a>] [<a href="https://arxiv.org/pdf/1912.00120">PDF</a>]
2. Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Matthew S.
Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba "Benchmarking Model-Based Reinforcement Learning", preprint. [<a href="https://arxiv.org/abs/1907.02057">arXiv</a>] [<a href="https://arxiv.org/pdf/1907.02057">PDF</a>]


